---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

<div class="content">
<p>
  I am currently a Postdoctoral Associate at the 
  <a href="https://nyuair.github.io/website/">NYU Embodied AI and Robotics Lab</a> at New York University (NYU) and the 
  <a href="https://nyuad.nyu.edu/en/research/faculty-labs-and-projects/center-for-artificial-intelligence-and-robotics.html">
    Center for AI and Robotics (CAIR)
  </a> at NYU Abu Dhabi (NYUAD), working under the supervision of <a href="https://nyuad.nyu.edu/en/academics/divisions/engineering/faculty/yi-fang.html"> Prof. Yi Fang</a>. 
  I am also a Visiting Research Fellow at the 
  <a href="https://wang.hms.harvard.edu/">Harvard AI and Robotics Lab</a> at Harvard University, 
  under the guidance of <a href="https://wang.hms.harvard.edu/team/dr-wang/"> Prof. Mengyu Wang</a>. I earned my Ph.D. in Cartography and Geographic Information System in 2021 from the 
  University of Chinese Academy of Sciences (UCAS) and the Aerospace Information Research Institute (AIR), 
  Chinese Academy of Sciences (CAS), where I was admitted through recommendation. 
  During my doctoral studies, I had the privilege of being a visiting scholar at the 
  <a href="https://ai4ce.github.io/">NYU AI4CE Lab</a> at New York University (NYU), mentored by <a href="https://engineering.nyu.edu/faculty/chen-feng"> Prof. Chen Feng</a>.
</p>


<p>My research is driven by a vision to create AI systems that are reliable, equitable, and beneficial to society. This mission guides my exploration of both foundational advancements in AI and their interdisciplinary applications. I focus on 2D/3D deep learning, multimodal large language models, and geospatial analysis, ensuring their trustworthy implementation in addressing complex challenges across computer vision, robotics, autonomous driving, remote sensing, healthcare, and urban studies. Committed to fostering trustworthy AI systems that prioritize accuracy, efficiency, robustness, and fairness, my work bridges theoretical innovation with practical impact. To date, I have published over 50 research papers in top-tier conferences such as NeurIPS, ICLR, ICCV, ECCV, AAAI, ICRA, and IROS, as well as leading journals like Science Advances, IEEE TVCG, IEEE TCSVT, IEEE TMM, IEEE TIM, IEEE GRSM, IEEE TGRS, ISPRS, and ESSD.</p>
</div>

<!-- # ğŸ”¥ News
- *2022.02*: &nbsp;ğŸ‰ğŸ‰ Congcong Wen dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2022.02*: &nbsp;ğŸ‰ğŸ‰ Congcong Wen dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
 -->
# ğŸ”¥ News
<div class="news-container">
  <ul class="news-list">
<<<<<<< HEAD
    <li>2025.07: &nbsp;ğŸ‰ğŸ‰ I will serve as an area chair for WACV 2026.</li>
    <li>2025.06: &nbsp;ğŸ‰ğŸ‰ One paper is accepted by ICCV 2025.</li>
    <li>2025.06: &nbsp;ğŸ‰ğŸ‰ Two papers are accepted by IROS 2025.</li>
    <li>2025.04: &nbsp;ğŸ‰ğŸ‰ One paper is accepted by ISPRS P&RS (Impact Factor: <strong style="color: red; font-weight: bold;">10.6</strong>).</li>
    <li>2025.03: &nbsp;ğŸ‰ğŸ‰ One paper is accepted by IEEE GRSM (Impact Factor: <strong style="color: red; font-weight: bold;">16.2</strong>).</li>
    <li>2025.03: &nbsp;ğŸ‰ğŸ‰ One paper is accepted by Science Advances (Impact Factor: <strong style="color: red; font-weight: bold;">11.7</strong>, Accept Rate < <strong style="color: red; font-weight: bold;">8.2%</strong>).</li>
    <li>2025.02: &nbsp;ğŸ‰ğŸ‰ One paper is accepted by IEEE TGRS.</li>
=======
    <li>2025.07: &nbsp;ğŸ‰ğŸ‰ One paper is accepted by Earth-Science Reviews (Impact Factor: <strong style="color: red; font-weight: bold;">10.0</strong>).</li>
    <li>2025.07: &nbsp;ğŸ‰ğŸ‰ One paper is accepted by Humanoids.</li>
    <li>2025.07: &nbsp;ğŸ‰ğŸ‰ I will serve as an area chair for WACV 2026.</li>
    <li>2025.06: &nbsp;ğŸ‰ğŸ‰ One paper (Wavelet Policy) is accepted by ICCV 2025.</li>
    <li>2025.06: &nbsp;ğŸ‰ğŸ‰ Two paper are accepted by IROS 2025.</li>
    <li>2025.04: &nbsp;ğŸ‰ğŸ‰ One paper (RSGPT) is accepted by ISPRS P&RS (Impact Factor: <strong style="color: red; font-weight: bold;">10.6</strong>).</li>
    <li>2025.03: &nbsp;ğŸ‰ğŸ‰ One paper (FedRSCLIP) is accepted by IEEE GRSM (Impact Factor: <strong style="color: red; font-weight: bold;">16.2</strong>).</li>
    <li>2025.03: &nbsp;ğŸ‰ğŸ‰ One paper (FairDiffusion) is accepted by Science Advances (Impact Factor: <strong style="color: red; font-weight: bold;">11.7</strong>, Accept Rate < <strong style="color: red; font-weight: bold;">8.2%</strong>).</li>
    <li>2025.02: &nbsp;ğŸ‰ğŸ‰ One paper (RS-MOE) is accepted by IEEE TGRS.</li>
>>>>>>> 1834ef5 (update)
    <li>2025.01: &nbsp;ğŸ‰ğŸ‰ One paper is accepted by ICLR 2025.</li>
    <li>2025.01: &nbsp;ğŸ‰ğŸ‰ One paper is accepted by IEEE TCSVT.</li>
    <li>2024.12: &nbsp;ğŸ‰ğŸ‰ One paper is accepted by ICASSP 2025.</li>
    <li>2024.12: &nbsp;ğŸ‰ğŸ‰ One paper is accepted by AAAI 2025.</li>
    <li>2024.12: &nbsp;ğŸ‰ğŸ‰ One paper is accepted by IEEE GRSL.</li>
    <li>2024.09: &nbsp;ğŸ‰ğŸ‰ One paper is accepted by NeurIPS 2024.</li>
    <li>2024.08: &nbsp;ğŸ‰ğŸ‰ Five papers are accepted by ICPR 2024, including one Oral.</li>
    <li>2024.06: &nbsp;ğŸ‰ğŸ‰ Two papers are accepted by IROS 2024.</li>
    <li>2024.06: &nbsp;ğŸ‰ğŸ‰ I serve as Session Co-Chair for ICRA 2024.</li>
    <li>2024.03: &nbsp;ğŸ‰ğŸ‰ One paper is accepted by IEEE GRSM (Impact Factor: <strong style="color: red; font-weight: bold;">16.2</strong>).</li>
    <li>2024.02: &nbsp;ğŸ‰ğŸ‰ One paper is accepted by ESSD (Impact Factor: <strong style="color: red; font-weight: bold;">11.2</strong>).</li>
    <li>2024.02: &nbsp;ğŸ‰ğŸ‰ One paper is accepted by IEEE TIM.</li>
    <li>2024.01: &nbsp;ğŸ‰ğŸ‰ One paper is accepted by ISPRS P&RS (Impact Factor: <strong style="color: red; font-weight: bold;">10.6</strong>).</li>
    <li>2024.09: &nbsp;ğŸ‰ğŸ‰ One paper is accepted by ICRA 2024.</li>
    <!-- æ›´å¤šå†…å®¹ -->
  </ul>
</div>


# ğŸ“ Publications 

<h2> ğŸ§  Multimodal Large Language Models</h2>




<div class='paper-box'> <div class='paper-box-image'> <div> <div class="badge">NeurIPS 2024</div> <img src='images/GAMAP_Neurips.png' alt="sym" width="100%"> </div> </div> <div class='paper-box-text' markdown="1">
[GAMap: Zero-Shot Object Goal Navigation with Multi-Scale Geometric-Affordance Guidance](https://neurips.cc/virtual/2024/poster/95755)

Shuaihang Yuan, Hao Huang, Yu Hao, **Congcong Wen**, Anthony Tzes, Yi Fang

<p><a href="https://shalexyuan.github.io/GAMap/"><strong>Project</strong></a> <a href="https://github.com/shalexyuan/GAMap"><strong>Code</strong></a> </p>


<p> ğŸ¤– Robotics</p>
</div> </div>



<div class='paper-box'> <div class='paper-box-image'> <div> <div class="badge">IROS 2025</div> <img src='images/HSACLLM.png' alt="sym" width="100%"> </div> </div> <div class='paper-box-text' markdown="1">
[Socially-Aware Robot Navigation Enhanced by Bidirectional Natural Language Conversations Using Large Language Models](https://arxiv.org/abs/2409.04965)

**Congcong Wen**, Yifan Liu, Geeta Chandra Raju Bethala, Hui Lin, Mengyu Wang, Yu-Shen Liu, Anthony Tzes, and Yi Fang

<p><a href="https://hsacllm.github.io/"><strong>Project</strong></a></p>

<p> ğŸ¤– Robotics</p>

</div> </div>


<div class='paper-box'> <div class='paper-box-image'> <div> <div class="badge">IEEE TGRS 2025</div> <img src='images/RSMOE.png' alt="sym" width="100%"> </div> </div> <div class='paper-box-text' markdown="1">
[RS-MoE: A Vision-Language Model with Mixture of Experts for Remote Sensing Image Captioning and Visual Question Answering](https://arxiv.org/abs/2411.01595)
Hui Lin, Danfeng Hong, Shuhang Ge, Chuyao Luo, Kai Jiang, Hao Jin, **Congcong Wenâ€ **.

<p><a href="https://github.com/CongcongWen1208/RS-MoE"><strong>Code</strong></a> </p>

<p> ğŸ›°ï¸ Remote Sensing</p>

</div> </div>

<div class='paper-box'> <div class='paper-box-image'> <div> <div class="badge">IEEE GRSM 2025</div> <img src='images/FedRSClip.png' alt="sym" width="100%"> </div> </div> <div class='paper-box-text' markdown="1">
[FedRSClip: Federated Learning for Remote Sensing Scene Classification Using Vision-Language Models](https://arxiv.org/abs/2501.02461)

Hui Lin, Chao Zhang, Danfeng Hong, Kexin Dong, **Congcong Wenâ€ **

<!-- <p><a href="https://github.com/lixiang-ucas/D-FCN"><strong>Code</strong></a> <strong><span class="show_paper_citations" data="DhtAFkwAAAAJ:ALROH1vI_8AC">| Citations: 118</span></strong></p> -->

<p> ğŸ›°ï¸ Remote Sensing</p>

</div> </div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IEEE GRSM 2024</div><img src='images/RSVLM_GRSM.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Vision-Language Models in Remote Sensing: Current Progress and Future Trends](https://ieeexplore.ieee.org/document/10506064)

Xiang Li\*, **Congcong Wen\***, Yuan Hu\*, Zhenghang Yuan, Xiao Xiang Zhu.

<p><strong><span class="show_paper_citations" data="DhtAFkwAAAAJ:ALROH1vI_8AC"> Citations: 92</span></strong></p>

<p> ğŸ›°ï¸ Remote Sensing</p>

</div>
</div>

<div class='paper-box'> <div class='paper-box-image'> <div> <div class="badge">ISPRS 2025</div> <img src='images/RSGPT_ArXiv.png' alt="sym" width="100%"> </div> </div> <div class='paper-box-text' markdown="1">
[RSGPT: A Remote Sensing Vision Language Model and Benchmark](https://arxiv.org/abs/2307.15266)

Yuan Hu, Jianlong Yuan, **Congcong Wen**, Xiaonan Lu, Xiang Li

<p><a href="https://github.com/Lavender105/RSGPT"><strong>Code</strong></a> <strong><span class="show_paper_citations" data="DhtAFkwAAAAJ:ALROH1vI_8AC">| Citations: 126</span></strong></p>

<p> ğŸ›°ï¸ Remote Sensing</p>

</div> </div>






<li><code class="language-plaintext highlighter-rouge">ICARA 2025</code> <a href="#">Integrating Retrospective Framework in Multi-Robot Collaboration.</a> J. Liang, H. Huang, Y. Hao, G. Bethala, <strong>C. Wen,</strong>, J., Rizzo, Y. Fang. ğŸ¤– Robotics </li>

<li><code class="language-plaintext highlighter-rouge">ICASSP 2025</code> <a href="#">A2GP-SF: Enhancing Few-Shot Class Incremental Learning via Attribute Generative Prompting and Adaptive Sharpness Flattening.</a> Z. Chen, D. Wang, S. Fu, <strong>C. Wen,</strong>, H. Lin, B. Chen.  ğŸ‘ï¸â€ğŸ—¨ï¸ Computer Vision </li>



<li><code class="language-plaintext highlighter-rouge">ICPR 2024</code> <a href="#">Zero-shot Object Navigation using Vision-Language Models.</a> <strong>C. Wen,</strong>, Y. Huang, H. Huang, Y. Huang, S. Yuan, Y. Hao, H. Lin, Y. Liu, Y. Fang. ğŸ¤– Robotics </li>

<li><code class="language-plaintext highlighter-rouge">ICPR 2024</code> <a href="#">Exploring the Reliability of Foundation Model-Based Frontier Selection in Zero-Shot Object Goal Navigation.</a> S. Yuan, H. Unlu, H. Huang, <strong>C. Wen,</strong>, A. Tzes, Y. Fang. ğŸ¤– Robotics </li>


<li><code class="language-plaintext highlighter-rouge">ICPR 2024</code> <a href="#">Reliable Semantic Understanding for Real-World Zero-Shot Object Goal Navigation.</a>  H. Unlu, S. Yuan, <strong>C. Wen,</strong>, H. Huang, A. Tzes, Y. Fang. ğŸ¤– Robotics </li>


<li><code class="language-plaintext highlighter-rouge">IROS 2024</code> <a href="https://ieeexplore.ieee.org/document/10801606">ChatMap: A Wearable Platform Based on the Multi-modal Foundation Model to Augment Spatial Cognition for People with Blindness and Low Vision.</a> Y. Hao, A. Magay, H. Huang, S. Yuan, <strong>C. Wen,</strong>, Y. Fang. ğŸ¤– Robotics, ğŸ¥ HealthCare </li>


<li><code class="language-plaintext highlighter-rouge">JAG 2023</code> <a href="https://www.sciencedirect.com/science/article/pii/S1569843223003217">RS-CLIP: Zero Shot Remote Sensing Scene Classification via Contrastive Vision-Language Supervision.</a> X. Li, <strong>C. Wen,</strong>, Y. Hu, N. Zhou. ğŸ›°ï¸ Remote Sensing </li>




<h2> ğŸ§± 3D Deep Learning</h2>




<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IEEE TMM 2023</div><img src='images/Adv_TMM.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[3D Shape Contrastive Representation Learning with Adversarial Examples.](https://ieeexplore.ieee.org/abstract/document/10094000)

**Congcong Wen**, Xiang Li, Hao Huang, Yu-Shen Liu, Yi Fang.

<!-- <p><a href="https://ai4ce.github.io/FLAT/"><strong>Project</strong></a> <a href="https://github.com/ai4ce/FLAT"><strong>Code</strong></a> <strong><span class="show_paper_citations" data="DhtAFkwAAAAJ:ALROH1vI_8AC">| Citations: 63</span></strong></p> -->

<p> ğŸ¤– Robotics, ğŸ‘ï¸â€ğŸ—¨ï¸ Computer Vision</p>

</div>
</div>




<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ISPRS 2021</div><img src='images/GACNN_ISPRS.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Airborne LiDAR Point Cloud Classification with Global-Local Graph Attention Convolution Neural Network.](https://www.sciencedirect.com/science/article/pii/S0924271621000071)

**Congcong Wen**, Xiang Li, Xiaojing Yao, Ling Peng, Tianhe Chi.

<p><strong><span class="show_paper_citations" data="DhtAFkwAAAAJ:ALROH1vI_8AC"> Citations: 89</span></strong></p>

<p> ğŸ›°ï¸ Remote Sensing, ğŸ¤– Robotics, ğŸš— Autonomous Driving</p>

</div>
</div>




<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ISPRS 2020</div><img src='images/DFCN_ISPRS.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Directionally Constrained Fully Convolutional Neural Network For Airborne Lidar Point Cloud Classification.](https://www.sciencedirect.com/science/article/abs/pii/S0924271620300381)

**Congcong Wen**, Lina Yang, Ling Peng, Xiang Li, Tianhe Chi.

<p><a href="https://github.com/lixiang-ucas/D-FCN"><strong>Code</strong></a> <strong><span class="show_paper_citations" data="DhtAFkwAAAAJ:ALROH1vI_8AC">| Citations: 118</span></strong></p>

<p> ğŸ›°ï¸ Remote Sensing, ğŸ¤– Robotics, ğŸš— Autonomous Driving</p>

</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IEEE TVCG 2019</div><img src='images/TCNET_TVCG.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Topology Constrained Shape Correspondence.](https://ieeexplore.ieee.org/document/9091324)

Xiang Li\*, **Congcong Wen\***, Lingjing Wang, Yi Fang.



<p><a href="https://github.com/lixiang-ucas/TP-Net"><strong>Code</strong></a> </p>

<p> ğŸ¤– Robotics, ğŸ‘ï¸â€ğŸ—¨ï¸ Computer Vision</p>

</div>
</div>



<li><code class="language-plaintext highlighter-rouge">IROS 2024</code> <a href="#">Weakly Scene Segmentation Using Efficient Transformer.</a> H. Huang, S. Yuan, <strong>C. Wen,</strong> Y. Hao, Y. Fang. ğŸ‘ï¸â€ğŸ—¨ï¸ Computer Vision </li>

<li><code class="language-plaintext highlighter-rouge">PRL 2024</code> <a href="#">Learning to Learn Point Signature for 3D Shape Geometry.</a> H. Huang, L. Wang, X. Li, S. Yuan, <strong>C. Wenâ€ ,</strong> Y. Hao, Y. Fang. ğŸ‘ï¸â€ğŸ—¨ï¸ Computer Vision </li> 


<li><code class="language-plaintext highlighter-rouge">ICRA 2024</code> <a href="#">Noisy Few-Shot 3D Point Cloud Scene Segmentation.</a> H. Huang, S. Yuan, <strong>C. Wen,</strong> Y. Hao, Y. Fang. ğŸ‘ï¸â€ğŸ—¨ï¸ Computer Vision </li> 

<li><code class="language-plaintext highlighter-rouge">ICARA 2024</code> <a href="#">3D Hierarchical Transformer for Shape Correspondence Learning.</a> H. Huang, S. Yuan, <strong>C. Wen,</strong> Y. Hao, Y. Fang. ğŸ‘ï¸â€ğŸ—¨ï¸ Computer Vision </li> 

<li><code class="language-plaintext highlighter-rouge">IEEE TIM 2024</code> <a href="#">Multi-Modal Features and Accurate Place Recognition with Robust Optimization for Lidar-Visual-Inertial SLAM.</a> X. Zhao, <strong>C. Wen,</strong> S. Prakhya, H. Yin, R. Zhou, Y. Sun, J. Xu, H. Bai, Y. Wang. ğŸ¤– Robotics, ğŸš— Autonomous Driving </li> 

<li><code class="language-plaintext highlighter-rouge">Computers & Graphics 2024</code> <a href="#">A Single 3D Shape Wavelet-Based Generative Model.</a> H. Huang, S. Yuan, Z. Peng, Y. Hao, <strong>C. Wenâ€ ,</strong> Y. Fang. ğŸ‘ï¸â€ğŸ—¨ï¸ Computer Vision </li> 

<li><code class="language-plaintext highlighter-rouge">ArXiv 2024</code> <a href="#">Evaluating Deep Clustering Algorithms on Non-Categorical 3D CAD Models.</a> S. Xiang, C. Tseng, <strong>C. Wen,</strong> D. Desai, Y. Kou, B. Starly, D. Panozzo, C. Feng. ğŸ‘ï¸â€ğŸ—¨ï¸ Computer Vision </li> 

<li><code class="language-plaintext highlighter-rouge">IEEE TMM 2023</code> <a href="#">Retrieval-Specific View Learning for Sketch-to-Shape Retrieval.</a> S. Yuan*, <strong>C. Wen*,</strong> Y.-S. Liu, Y. Fang. ğŸ‘ï¸â€ğŸ—¨ï¸ Computer Vision </li> 

<li><code class="language-plaintext highlighter-rouge">ICRA 2023</code> <a href="#">Pyramid Learnable Tokens for 3D LiDAR Place Recognition.</a> <strong>C. Wen,</strong> H. Huang, Y. Liu, Y. Fang. ğŸ¤– Robotics, ğŸ‘ï¸â€ğŸ—¨ï¸ Computer Vision, ğŸš— Autonomous Driving </li> 

<li><code class="language-plaintext highlighter-rouge">ACCV 2022</code> <a href="#">Unsupervised 3D Shape Representation Learning Using Normalizing Flow.</a> X. Li*, <strong>C. Wen*,</strong> H. Huang, Y. Fang. ğŸ‘ï¸â€ğŸ—¨ï¸ Computer Vision </li> 

<li><code class="language-plaintext highlighter-rouge">ISPRS 2020</code> <a href="#">Density-Aware Convolutional Networks with Context Encoding for Airborne LiDAR Point Cloud Classification.</a> X. Li, L. Wang, M. Wang, <strong>C. Wen,</strong> N. Zhou, Y. Fang. ğŸ›°ï¸ Remote Sensing, ğŸ¤– Robotics, ğŸš— Autonomous Driving </li>



<h2> ğŸ–¼ï¸ 2D Deep Learning</h2>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IEEE TCSVT 2025</div><img src='images/GEFSOD_TCSVT.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Generalization-Enhanced Few-Shot Object Detection in Remote Sensing.](https://ieeexplore.ieee.org/abstract/document/10836905)

Hui Lin, Nan Li, Pengjuan Yao, Kexin Dong, Yuhan Guo, Danfeng Hong, Ying Zhang, **Congcong Wenâ€ **.

<!-- <p><strong><span class="show_paper_citations" data="DhtAFkwAAAAJ:ALROH1vI_8AC"> Citations: 415</span></strong></p> -->

<p><a href="https://github.com/leenamx/GE-FSOD"><strong>Code</strong></a> </p>

<p> ğŸ›°ï¸ Remote Sensing</p>


</div>
</div>




<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ISPRS 2024</div><img src='images/Realcity3D_ISPRS.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[AutoEncoding Tree for City Layout Generation and Applications.](https://arxiv.org/pdf/2309.15941.pdf)

Wenyu Han, **Congcong Wenâ€ **, Lazarus Chok, Yan Liang Tan, Sheung Lung Chan, Hang Zhao, Chen Feng.

<p><a href="https://ai4ce.github.io/RealCity3D/"><strong>Project</strong></a> <a href="https://github.com/ai4ce/RealCity3D"><strong>Code</strong></a> </p>



<p>ğŸ™ï¸ Urban Studies</p>
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IEEE RSGL 2024</div><img src='images/GRSL_2021.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Geometry-Aware Segmentation of Remote Sensing Images via Joint Height Estimation.](https://ieeexplore.ieee.org/abstract/document/9361998?casa_token=IhdFOUYi5-sAAAAA:q6lizg6LXDTingCDZn36M6QMpN67eIoLe_OMnu9VGEoJmgWEsXdjMYOWocYx6L4lI6OMex5UsQ)

 Xiang Li, **Congcong Wen**, Lingjing Wang, Yi Fang.

<p>ğŸ›°ï¸ Remote Sensing</p>
</div>
</div>


<li><code class="language-plaintext highlighter-rouge">ArXiv 2024</code> <a href="#">Equitable Deep Learning for Diabetic Retinopathy Detection Using Multi-Dimensional Retinal Imaging with Fair Adaptive Scaling: A Retrospective Study.</a> M. Shi*, M.M. Afzal*, H. Huang*, Y. Luo*, <strong>C. Wen*</strong>, M.O. Khan, Y. Tian, L. Kim. ğŸ¥ HealthCare </li> 

<!-- <li><code class="language-plaintext highlighter-rouge">Science Advances (Major Revision)</code> <a href="#">FairDiffusion: Enhancing Equity in Latent Diffusion Models via Fair Bayesian Perturbation.</a> Y. Luo*, M. Khan*, M. Afzal*, T. Wuermeling, M. Shi, Y. Tian, Y. Fang, <strong>C. Wen*</strong>, M. Wang. ğŸ¥ HealthCare </li>  -->

<li><code class="language-plaintext highlighter-rouge">ArXiv 2024</code> <a href="#">Impact of Data Distribution on Fairness Guarantees in Equitable Deep Learning.</a> Y. Luo*, <strong>C. Wen*</strong>, M. Shi, H. Huang, M. Li, Y. Fang, M. Wang, . ğŸ¥ HealthCare </li> 

<!-- <li><code class="language-plaintext highlighter-rouge">IEEE TCSVT 2025</code> <a href="#">Generalization-Enhanced Few-Shot Object Detection in Remote Sensing.</a> H. Lin, N. Li, P. Yao, K. Dong, D. Hong, <strong>C. Wen*</strong>. ğŸ›°ï¸ Remote Sensing </li>  -->

<!-- <li><code class="language-plaintext highlighter-rouge">ECCV 2024</code> <a href="#">FairDomain: Achieving Fairness in Cross-Domain Medical Image Classification and Segmentation.</a> Y. Tian*, <strong>C. Wen*</strong>, M. Shi, M.M. Afzal, H. Huang, O. Khan, Y. Luo, Y. Fang, M. Wang. ğŸ¥ HealthCare </li>  -->

<li><code class="language-plaintext highlighter-rouge">ICPR 2024</code> <a href="#">Goal-Driven Transformer for Robot Behavior Learning from Play Data.</a> <strong>C. Wen</strong>, J. Liang, S. Yuan, H. Huang, Y. Hao, H. Lin, Y. Liu, Y. Fang. ğŸ¤– Robotics </li> 

<li><code class="language-plaintext highlighter-rouge">ICPR 2024</code> <a href="#">Optimizing Personalized Robot Actions with Ranking of Trajectories.</a> H. Huang, Y. Liu, S. Yuan, <strong>C. Wen</strong>, Y. Hao, Y. Fang. ğŸ¤– Robotics </li> 

<!-- <li><code class="language-plaintext highlighter-rouge">ISPRS 2024</code> <a href="#">AutoEncoding Tree for City Layout Generation and Applications.</a> W. Han, <strong>C. Wen*</strong>, H. Zhao, C. Feng. ğŸ›°ï¸ Remote Sensing </li>  -->

<li><code class="language-plaintext highlighter-rouge">GIScience & Remote Sensing 2023</code> <a href="#">A Shape-Attention Pivot-Net for Identifying Central Pivot Irrigation Systems from Satellite Images Using a Cloud Computing Platform: An Application in the Contiguous US.</a> F. Tian, B. Wu, H. Zeng, M. Zhang, Y. Hu, Y. Xie, <strong>C. Wen</strong>, Z. Wang, X. Qin, W. Han, H. Yang. ğŸ›°ï¸ Remote Sensing </li> 

<!-- <li><code class="language-plaintext highlighter-rouge">IEEE GRSL 2021</code> <a href="#">Geometry-Aware Segmentation of Remote Sensing Images via Joint Height Estimation.</a> X. Li, <strong>C. Wen</strong>, L. Wang, Y. Fang. ğŸ›°ï¸ Remote Sensing </li>  -->

<li><code class="language-plaintext highlighter-rouge">IEEE GRSL 2020</code> <a href="#">Road Extraction from Remote Sensing Images in Wildland-Urban Interface Areas.</a> R. Chen, X. Li, Y. Hu, <strong>C. Wen</strong>, L. Peng. ğŸ›°ï¸ Remote Sensing </li> 

<li><code class="language-plaintext highlighter-rouge">ICAEES 2015</code> <a href="#">Urban Environmental Pulsation Research Based on Wavelet Decomposition Enhanced ARMA Model.</a> X. Li, L. Peng, T. Chi, <strong>C. Wen</strong>, Y. Xu. ğŸ›°ï¸ Remote Sensing, ğŸ™ï¸ Urban Studies </li>

<h2> ğŸŒ Trustworthy AI </h2>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Science Advances 2025</div><img src='images/FairDiff.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[FairDiffusion: Enhancing Equity in Latent Diffusion Models via Fair Bayesian Perturbation.](https://arxiv.org/abs/2412.20374)

Yan Luo\*, Muhammad Osama Khan\*, **Congcong Wen\***, Muhammad Muneeb Afzal, Titus Fidelis Wuermeling, Min Shi, Yu Tian, Yi Fang, Mengyu Wang.

<!-- <p><strong><span class="show_paper_citations" data="DhtAFkwAAAAJ:ALROH1vI_8AC"> Citations: 415</span></strong></p> -->


<p><a href="https://github.com/luoyan407/FairDiffusion"><strong>Code</strong></a>  <a href="https://drive.google.com/drive/folders/16VCgkvMdCM7UFRlhTlEa0oV2lxCho6Al?usp=sharing"><strong>Dataset</strong></a> </p>

<p> ğŸ¥ HealthCare</p>

</div></div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ECCV 2024</div><img src='images/FairDomain_ECCV.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Fairdomain: Achieving fairness in cross-domain medical image segmentation and classification.](https://link.springer.com/chapter/10.1007/978-3-031-73116-7_15)

Yu Tian\*, **Congcong Wen\***, Min Shi, Muhammad Muneeb Afzal, Hao Huang, Muhammad Osama Khan, Yan Luo, Yi Fang, Mengyu Wang.

<!-- <p><strong><span class="show_paper_citations" data="DhtAFkwAAAAJ:ALROH1vI_8AC"> Citations: 415</span></strong></p> -->

<p><a href="https://github.com/Harvard-Ophthalmology-AI-Lab/FairDomain"><strong>Code</strong></a> <a href="https://drive.google.com/drive/folders/1huH93JVeXMj9rK6p1OZRub868vv0UK0O?usp=drive_link"><strong>Dataset</strong></a> </p>

<p> ğŸ¥ HealthCare</p>

</div></div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ArXiv 2024</div><img src='images/TripleMixer_Arxiv.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[TripleMixer: A 3D Point Cloud Denoising Model for Adverse Weather.](https://arxiv.org/pdf/2408.13802)

Xiongwei Zhao\*, **Congcong Wen\***, Yang Wang, Haojie Bai, Wenhao Dou

<p><a href="https://github.com/Grandzxw/TripleMixer"><strong>Code</strong></a></p>

<p> ğŸ¤– Robotics, ğŸš— Autonomous Driving</p>


</div>
</div>


<div class='paper-box'> <div class='paper-box-image'> <div> <div class="badge">ArXiv 2024</div> <img src='images/LLMAttack.png' alt="sym" width="100%"> </div> </div> <div class='paper-box-text' markdown="1">
[How Secure Are Large Language Models (LLMs) for Navigation in Urban Environments?](https://arxiv.org/abs/2402.09546)

**Congcong Wen**, Jiazhao Liang, Shuaihang Yuan, Hao Huang, Yi Fang


<p> ğŸ¤– Robotics</p>

</div> </div>





<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICCV 2021 Oral</div><img src='images/FLAT_ICCV.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Fooling LiDAR Perception via Adversarial Trajectory Perturbation.](https://arxiv.org/pdf/2103.15326.pdf)

Yiming Li\*, **Congcong Wen\***, Felix Juefei-Xu, Chen Feng

<p><a href="https://ai4ce.github.io/FLAT/"><strong>Project</strong></a> <a href="https://github.com/ai4ce/FLAT"><strong>Code</strong></a> <strong><span class="show_paper_citations" data="DhtAFkwAAAAJ:ALROH1vI_8AC">| Citations: 63</span></strong></p>

<p> ğŸ¤– Robotics, ğŸš— Autonomous Driving</p>

</div>
</div>

<li><code class="language-plaintext highlighter-rouge">AAAI 2025</code> <a href="#">Towards Robust Visual Question Answering via Prompt-Driven Geometric Harmonization.</a> Y, Liu, J. Zhu, <strong>C. Wen,</strong>, G, Lu, H. Lin, B., Chen. ğŸ‘ï¸â€ğŸ—¨ï¸ Computer Vision </li>


<h2> ğŸŒ Geospatial Analysis </h2>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">STOEN 2019</div><img src='images/LSTME_SOTE.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[A Novel Spatiotemporal Convolutional Long Short-term Neural Network For Air Pollution Prediction.](https://www.sciencedirect.com/science/article/abs/pii/S0048969718344413)

**Congcong Wen**, Shufu Liu, Xiaojing Yao, Ling Peng, Xiang Li, Yuan Hu, Tianhe Chi.

<p><strong><span class="show_paper_citations" data="DhtAFkwAAAAJ:ALROH1vI_8AC"> Citations: 415</span></strong></p>
<p>ğŸ™ï¸ Urban Studies</p>

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IJGIS 2018</div><img src='images/IJGIS.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[A Spatial Co-Location Mining Algorithm that Includes Adaptive Proximity Improvements and Distant Instance References.](https://www.tandfonline.com/doi/full/10.1080/13658816.2018.1431839)

Xiaojing Yao, Liujia Chen, **Congcong Wen**, Ling Peng, Liang Yang, Tianhe Chi, Xiaomeng Wang, Wenhao Yu.

<p><strong><span class="show_paper_citations" data="DhtAFkwAAAAJ:ALROH1vI_8AC"> Citations: 29</span></strong></p>
<p>ğŸ™ï¸ Urban Studies</p>
</div>
</div>

<li><code class="language-plaintext highlighter-rouge">IEEE GRSL 2024</code> <a href="#">Earthquake Spatio-temporal Patterns around Precursory Thermal Infrared (TIR) Anomalies.</a> H. Lin, Y. Zhang, J. Zhou, X. Wang, H. Jin, <strong>C. Wenâ€ </strong>. ğŸ›°ï¸ Remote Sensing </li>

<li><code class="language-plaintext highlighter-rouge">Sensors and Materials 2024</code> <a href="#">Analysis of the Spatial Heterogeneity of Commuting Flows in Beijing: Perspectives from Mobile Phone Data.</a> S. Guo, Q. Huang, <strong>C. Wenâ€ </strong>. ğŸ™ï¸ Urban Studies </li>

<li><code class="language-plaintext highlighter-rouge">ESSD 2024</code> <a href="#">A Hydrogeomorphic Dataset for Characterizing Catchment Hydrological Behavior across the Tibetan Plateau.</a> Y. Guo, H. Zheng, Y. Yang, Y. Sang, <strong>C. Wen</strong>. ğŸ›°ï¸ Remote Sensing </li>

<li><code class="language-plaintext highlighter-rouge">IJWF 2020</code> <a href="#">An Improved VDIâ€“BSI Approach for Mapping Cropland Burned Area Using Combined Data from Landsat 8 Multispectral and Thermal Infrared Bands.</a> S. Liu, S. Wang, T. Chi, <strong>C. Wen</strong>, T. Wu, D. Wang. ğŸ›°ï¸ Remote Sensing </li>

<!-- <li><code class="language-plaintext highlighter-rouge">Science of The Total Environment 2019</code> <a href="#">A Novel Spatiotemporal Convolutional Long Short-Term Neural Network for Air Pollution Prediction.</a> <strong>C. Wen</strong>, S. Liu, X. Yao, L. Peng, X. Li. ğŸŒ Environmental Science </li> -->

<li><code class="language-plaintext highlighter-rouge">Journal of Geo-Information Science 2019</code> <a href="#">VLC and PDR Fusion Positioning by Incorporating High-Precision Indoor Map.</a> Y. Cheng, L. Peng, J. Wang, <strong>C. Wen</strong>, R. Chen. ğŸ™ï¸ Urban Studies </li>

<li><code class="language-plaintext highlighter-rouge">Journal of Computer Applications 2019</code> <a href="#">Improved A* Algorithm and Its Application in Indoor Robot Path Planning.</a> R. Chen, <strong>C. Wen</strong>, L. Peng, C. You. ğŸ¤– Robotics </li>

<li><code class="language-plaintext highlighter-rouge">IVPAI 2018</code> <a href="#">Research on Indoor Visible Light Positioning System Based on SLAM Maps.</a> <strong>C. Wen</strong>, C. You, L. Peng, T. Wu, B. Song, B. Lv. ğŸ™ï¸ Urban Studies </li>

<!-- <li><code class="language-plaintext highlighter-rouge">IJGIS 2018</code> <a href="#">A Spatial Co-Location Mining Algorithm that Includes Adaptive Proximity Improvements and Distant Instance References.</a> X. Yao, L. Chen, <strong>C. Wen</strong>, L. Peng, L. Yang, T. Chi, X. Wang, W. Yu. ğŸ“Š Data Mining </li> -->

<li><code class="language-plaintext highlighter-rouge">Journal of Geo-Information Science 2018</code> <a href="#">Topic Model Combined with the SVM for Small Scale Land Use Classification.</a> <strong>C. Wen</strong>, L. Peng, L. Yang, T. Chi. ğŸ™ï¸ Urban Studies </li>

<li><code class="language-plaintext highlighter-rouge">Progress In Geography 2017</code> <a href="#">Relationship between Travel Behavior and Income Level of Urban Residents: A Case Study in Shanghai Municipality.</a> S. Guo, <strong>C. Wen</strong>, Y. He, T. Per. ğŸ™ï¸ Urban Studies </li>

<li><code class="language-plaintext highlighter-rouge">Fire Science And Technology 2017</code> <a href="#">Mobile Patrol System of Firefighting Facilities Based on BLE and RFID.</a> S. Li, L. Peng, L. Yu, H. Lin, T. Chi, <strong>C. Wen</strong>. ğŸ™ï¸ Urban Studies </li>

<li><code class="language-plaintext highlighter-rouge">Fire Science And Technology 2017</code> <a href="#">The Building of Integrated Inspection System of Indoor and Outdoor Fire Facilities.</a> S. Cui, L. Peng, S. Li, H. Lin, <strong>C. Wen</strong>. ğŸ™ï¸ Urban Studies </li>




<!-- <h2 id="-speech-synthesis"> ğŸ¤– Robotics</h2>

<h2 id="-speech-synthesis"> ğŸŒ Remote Sensing</h2>

<h2 id="-speech-synthesis"> ğŸ¥ HealthCare</h2>

<h2 id="-speech-synthesis"> ğŸ“‘ Geospatial Analysis</h2>

<h2 id="-speech-synthesis"> ğŸ™ Ubran Planing</h2> -->








<!-- <li><code class="language-plaintext highlighter-rouge">AAAI 2024</code> <a href="#">Emotion Rendering for Conversational Speech Synthesis with Heterogeneous Graph-Based Context Modeling</a>, Rui Liu, Yifan Hu, <strong>Yi Ren</strong>, et al. <a href="https://github.com/walker-hyf/ECSS"><img src="https://img.shields.io/github/stars/walker-hyf/ECSS?style=social&amp;label=Code+Stars" alt=""></a></li>

- [Congcong Wen dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet](https://github.com), A, B, C, **CVPR 2020** -->



# ğŸ“– Academic Services

<h2>Senior Academic Services:</h2>
  <h3>Journal:</h3>
  <ul>
    <li>Associate Editor, GIScience</li>
    <li>Guest Editor, Journal of Environmental &amp; Earth Sciences</li>
  </ul>
  <h3>Conference:</h3>
  <ul>
    <li>Session Co-Chair, IEEE International Conference on Robotics and Automation (ICRA) 2024</li>
    <li>Program Committee Member, Association for the Advancement of Artificial Intelligence (AAAI), 2025</li>
 </ul>


<h2>Reviewer</h2>
  <h3>Journal Review:</h3>
  <p>
  Nature Machine Intelligence (NMI), IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), Pattern Recognition (PR), International Journal of Computer Vision (IJCV), IEEE Transactions on Image Processing (TIP), 
  IEEE Transactions on Visualization and Computer Graphics (TVCG), ISPRS Journal of Photogrammetry and Remote Sensing (ISPRS P&amp;RS), 
  IEEE Transactions on Geoscience and Remote Sensing (IEEE TGRS), etc.
</p>

  <h3>Conference Review:</h3>
  <p>
  CVPR 2022/2023/2024, ECCV 2022/2024, ICCV 2023, NeurIPS 2022/2023/2024, ICLR 2023/2024/2025, 
  AAAI 2023/2024, ICRA 2023/2024, IROS 2023/2024, etc.
</p>

<!-- # ğŸ’¬ Invited Talks
- *2021.06*, Congcong Wen dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2021.03*, Congcong Wen dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  \| [\[video\]](https://github.com/) -->

# ğŸ’» Teaching
- *Spring 2024*, Computer Vision and Pattern Recognition, New York University Abu Dhabi.

# ğŸ– Honors and Awards
- *2023* NYUAD Postdoctoral Travel Award
- *2021* Chinese Academy of Sciences Deanâ€™s Excellent Award
- *2021* Outstanding Graduates Award of Beijing City
- *2021* Outstanding Graduates Award of the Chinese Academy of Sciences
- *2021* Pacemaker to Merit Student, University of Chinese Academy of Sciences
- *2020* National Scholarship
- *2020* Outstanding Academic Scholarship for Graduate Student
- *2019* China Scholarship Council Scholarship
- *2019* Outstanding Research Paper Award
- *2018* First Prize in PIE Remote Sensing Software Secondary Development Competition
- *2017* Merit Student, University of Chinese Academy of Sciences
- *2016* Chinese Academy of Sciences Scholarship
- *2016* Outstanding Graduates Award
- *2016* Outstanding Graduation Thesis Award
- *2015* National Scholarship
- *2014* China University of Petroleum Scholarship
- *2014* Surveying and Mapping Department Scholarship
- *2014* Second Prize of The Fourth National University GIS Application Skills Competition
- *2013* Outstanding Student
- *2013* Outstanding Student Cadre